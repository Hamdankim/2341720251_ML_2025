{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamdankim/2341720251_ML_2025/blob/main/JS13/P1_JS13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a225e12a",
      "metadata": {
        "id": "a225e12a"
      },
      "source": [
        "# Praktikum 1 – JST XOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c476b6e5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c476b6e5",
        "outputId": "c43a68b7-800e-4285-957d-dc2f824bca82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.2868014254954371\n",
            "Epoch 200, Loss: 0.24805047767615995\n",
            "Epoch 400, Loss: 0.24577761931768316\n",
            "Epoch 600, Loss: 0.24227995414627174\n",
            "Epoch 800, Loss: 0.23669358973883864\n",
            "Epoch 1000, Loss: 0.22850222352779442\n",
            "Epoch 1200, Loss: 0.21812191998780678\n",
            "Epoch 1400, Loss: 0.20674082877414285\n",
            "Epoch 1600, Loss: 0.19505736984795016\n",
            "Epoch 1800, Loss: 0.18207294011537\n",
            "Epoch 2000, Loss: 0.16426489173182518\n",
            "Epoch 2200, Loss: 0.13695399134829278\n",
            "Epoch 2400, Loss: 0.10295307547741907\n",
            "Epoch 2600, Loss: 0.07241294652746641\n",
            "Epoch 2800, Loss: 0.05075402379056525\n",
            "Epoch 3000, Loss: 0.03679810268888732\n",
            "Epoch 3200, Loss: 0.027836895923755708\n",
            "Epoch 3400, Loss: 0.021889422034523372\n",
            "Epoch 3600, Loss: 0.017775969474165827\n",
            "Epoch 3800, Loss: 0.0148173751398731\n",
            "Epoch 4000, Loss: 0.012615131603454897\n",
            "Epoch 4200, Loss: 0.0109271594225803\n",
            "Epoch 4400, Loss: 0.009600809694999584\n",
            "Epoch 4600, Loss: 0.008536350753458214\n",
            "Epoch 4800, Loss: 0.00766649100773633\n",
            "\n",
            "Prediksi akhir (Sigmoid):\n",
            "[[0.09474266]\n",
            " [0.91958643]\n",
            " [0.92157109]\n",
            " [0.07872491]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3   # diubah dari 2 → 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi bobot\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# Aktivasi sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "loss_history_sigmoid = []\n",
        "\n",
        "# Training\n",
        "for epoch in range(5000):\n",
        "    # Forward\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = sigmoid(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history_sigmoid.append(loss)\n",
        "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "print(\"\\nPrediksi akhir (Sigmoid):\")\n",
        "print(a2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2ad6180c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ad6180c",
        "outputId": "c5d20264-88e8-4fac-f425-9f1d0d07bda1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss (ReLU): 0.398755072416237\n",
            "Epoch 200, Loss (ReLU): 0.24633858983456644\n",
            "Epoch 400, Loss (ReLU): 0.2060779852426214\n",
            "Epoch 600, Loss (ReLU): 0.17435459668162073\n",
            "Epoch 800, Loss (ReLU): 0.16926412565471016\n",
            "Epoch 1000, Loss (ReLU): 0.1677753690901775\n",
            "Epoch 1200, Loss (ReLU): 0.16743757025579406\n",
            "Epoch 1400, Loss (ReLU): 0.16723350782460317\n",
            "Epoch 1600, Loss (ReLU): 0.16711405262818177\n",
            "Epoch 1800, Loss (ReLU): 0.16697902364348238\n",
            "Epoch 2000, Loss (ReLU): 0.166937078838057\n",
            "Epoch 2200, Loss (ReLU): 0.16684775140981153\n",
            "Epoch 2400, Loss (ReLU): 0.16686740992508114\n",
            "Epoch 2600, Loss (ReLU): 0.16682740565480017\n",
            "Epoch 2800, Loss (ReLU): 0.16680532348201862\n",
            "Epoch 3000, Loss (ReLU): 0.16679803324729178\n",
            "Epoch 3200, Loss (ReLU): 0.1667927617711978\n",
            "Epoch 3400, Loss (ReLU): 0.16677028689751752\n",
            "Epoch 3600, Loss (ReLU): 0.16675373571791843\n",
            "Epoch 3800, Loss (ReLU): 0.1667622563459775\n",
            "Epoch 4000, Loss (ReLU): 0.16675260813547416\n",
            "Epoch 4200, Loss (ReLU): 0.1667439858896652\n",
            "Epoch 4400, Loss (ReLU): 0.16673933178590933\n",
            "Epoch 4600, Loss (ReLU): 0.16673457383534296\n",
            "Epoch 4800, Loss (ReLU): 0.16673271130634523\n",
            "\n",
            "Prediksi akhir (ReLU):\n",
            "[[0.66627742]\n",
            " [0.66627742]\n",
            " [0.66627742]\n",
            " [0.01595716]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "# Parameter\n",
        "input_size = 2\n",
        "hidden_size = 3\n",
        "output_size = 1\n",
        "lr = 0.1\n",
        "\n",
        "# Inisialisasi\n",
        "W1 = np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros((1, hidden_size))\n",
        "W2 = np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros((1, output_size))\n",
        "\n",
        "# ReLU\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "# Sigmoid output\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "loss_history_relu = []\n",
        "\n",
        "# Training\n",
        "for epoch in range(5000):\n",
        "    # Forward\n",
        "    z1 = np.dot(X, W1) + b1\n",
        "    a1 = relu(z1)\n",
        "    z2 = np.dot(a1, W2) + b2\n",
        "    a2 = sigmoid(z2)\n",
        "\n",
        "    # Error\n",
        "    error = y - a2\n",
        "\n",
        "    # Backprop\n",
        "    d_a2 = error * sigmoid_derivative(a2)\n",
        "    d_W2 = np.dot(a1.T, d_a2)\n",
        "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
        "\n",
        "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
        "    d_W1 = np.dot(X.T, d_a1)\n",
        "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
        "\n",
        "    # Update\n",
        "    W1 += lr * d_W1\n",
        "    b1 += lr * d_b1\n",
        "    W2 += lr * d_W2\n",
        "    b2 += lr * d_b2\n",
        "\n",
        "    if epoch % 200 == 0:\n",
        "        loss = np.mean(np.square(error))\n",
        "        loss_history_relu.append(loss)\n",
        "        print(f\"Epoch {epoch}, Loss (ReLU): {loss}\")\n",
        "\n",
        "print(\"\\nPrediksi akhir (ReLU):\")\n",
        "print(a2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cff1b78"
      },
      "source": [
        "# Praktikum 1 - Tugas 1\n",
        "\n",
        "Ubah hidden layer jadi 3 neuron, bandingkan loss, dan tambahkan ReLU."
      ],
      "id": "6cff1b78"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07497613",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8784169-5397-4a4b-e288-9cf0e66e1d96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 2 neuron: [np.float64(0.25701987209608385), np.float64(0.24993321464702528), np.float64(0.24988778846546375), np.float64(0.249802291766035), np.float64(0.2496185789830107), np.float64(0.24915630447393267), np.float64(0.24779901732011578), np.float64(0.2436741346221472), np.float64(0.23320809736030157), np.float64(0.21483725554736133)]\n",
            "Loss 3 neuron: [np.float64(0.3300692255976765), np.float64(0.23271588252709105), np.float64(0.20886325898198888), np.float64(0.18215490272578094), np.float64(0.1519318566339078), np.float64(0.09873786825132458), np.float64(0.050188102088606155), np.float64(0.02719567747272042), np.float64(0.017082029224693367), np.float64(0.01197369281787037)]\n",
            "Loss ReLU: [np.float64(0.24961459045948817), np.float64(0.12435950525101297), np.float64(0.006647428662457552), np.float64(0.0031088606598476606), np.float64(0.0019657827658700645), np.float64(0.0014187430233072989), np.float64(0.0011018577654301298), np.float64(0.0008966176006552336), np.float64(0.0007537023540151156), np.float64(0.0006485171334326496)]\n",
            "Prediksi ReLU: [[0.02989887]\n",
            " [0.98444316]\n",
            " [0.98435824]\n",
            " [0.02989887]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Dataset XOR\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([[0],[1],[1],[0]])\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def train(hidden_size, activation='sigmoid', epochs=5000):\n",
        "    input_size, output_size, lr = 2, 1, 0.1\n",
        "    W1 = np.random.randn(input_size, hidden_size)\n",
        "    b1 = np.zeros((1, hidden_size))\n",
        "    W2 = np.random.randn(hidden_size, output_size)\n",
        "    b2 = np.zeros((1, output_size))\n",
        "\n",
        "    act = sigmoid if activation=='sigmoid' else relu\n",
        "    act_der = sigmoid_derivative if activation=='sigmoid' else relu_derivative\n",
        "\n",
        "    losses=[]\n",
        "    for epoch in range(epochs):\n",
        "        z1 = X.dot(W1) + b1\n",
        "        a1 = act(z1)\n",
        "        z2 = a1.dot(W2) + b2\n",
        "        a2 = sigmoid(z2)\n",
        "\n",
        "        error = y - a2\n",
        "        loss = np.mean(error**2)\n",
        "        if epoch%500==0: losses.append(loss)\n",
        "\n",
        "        d_a2 = error * sigmoid_derivative(a2)\n",
        "        d_W2 = a1.T.dot(d_a2)\n",
        "        d_b2 = d_a2.sum(axis=0, keepdims=True)\n",
        "\n",
        "        d_a1 = d_a2.dot(W2.T) * act_der(a1)\n",
        "        d_W1 = X.T.dot(d_a1)\n",
        "        d_b1 = d_a1.sum(axis=0, keepdims=True)\n",
        "\n",
        "        W1 += lr*d_W1; b1 += lr*d_b1\n",
        "        W2 += lr*d_W2; b2 += lr*d_b2\n",
        "\n",
        "    return losses, a2\n",
        "\n",
        "loss_2, pred_2 = train(2,'sigmoid')\n",
        "loss_3, pred_3 = train(3,'sigmoid')\n",
        "loss_relu, pred_relu = train(3,'relu')\n",
        "\n",
        "print(\"Loss 2 neuron:\", loss_2)\n",
        "print(\"Loss 3 neuron:\", loss_3)\n",
        "print(\"Loss ReLU:\", loss_relu)\n",
        "print(\"Prediksi ReLU:\", pred_relu)\n"
      ],
      "id": "07497613"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}